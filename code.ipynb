{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051c37e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intenship\\cctv video processing\\myenv\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intenship\\cctv video processing\\myenv\\Lib\\site-packages\\torchvision\\models\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01malexnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdensenet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intenship\\cctv video processing\\myenv\\Lib\\site-packages\\torchvision\\models\\alexnet.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_model, Weights, WeightsEnum\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_meta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _IMAGENET_CATEGORIES\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ovewrite_named_param, handle_legacy_interface\n\u001b[32m     14\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mAlexNet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAlexNet_Weights\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33malexnet\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAlexNet\u001b[39;00m(nn.Module):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intenship\\cctv video processing\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional, TypeVar, Union\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sequence_to_str\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WeightsEnum\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIntermediateLayerGetter\u001b[39;00m(nn.ModuleDict):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1178\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1140\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1080\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1504\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1476\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1645\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:161\u001b[39m, in \u001b[36m_path_isfile\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:153\u001b[39m, in \u001b[36m_path_is_mode_type\u001b[39m\u001b[34m(path, mode)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa39ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCTVDetection:\n",
    "    def __init__(self, rtsp_url):\n",
    "        \"\"\"\n",
    "        Initialize the CCTV detection system\n",
    "        \n",
    "        Args:\n",
    "            rtsp_url (str): RTSP URL of the CCTV camera\n",
    "        \"\"\"\n",
    "        self.rtsp_url = rtsp_url\n",
    "        self.cap = None\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        \n",
    "        # Initialize the detection model\n",
    "        self.initialize_model()\n",
    "        \n",
    "        # Define color mappings for different classes\n",
    "        self.colors = {\n",
    "            'car': (0, 255, 0),        # Green\n",
    "            'bike': (255, 0, 0),       # Blue\n",
    "            'cycle': (0, 255, 255),    # Yellow\n",
    "            'motorcycle': (255, 0, 255), # Magenta\n",
    "            'male': (0, 165, 255),     # Orange\n",
    "            'female': (255, 192, 203), # Pink\n",
    "            'kid': (255, 255, 0)       # Cyan\n",
    "        }\n",
    "        \n",
    "        # Class mappings\n",
    "        self.vehicle_classes = ['car', 'bike', 'cycle', 'motorcycle']\n",
    "        self.person_classes = ['male', 'female', 'kid']\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize YOLOv5 model for object detection\"\"\"\n",
    "        try:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Using device: {self.device}\")\n",
    "            \n",
    "            # Load YOLOv5 model (you can use different sizes: n, s, m, l, x)\n",
    "            self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "            self.model.to(self.device)\n",
    "            self.model.conf = 0.5  # Confidence threshold\n",
    "            self.model.iou = 0.45  # NMS IoU threshold\n",
    "            \n",
    "            print(\"Model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def map_yolo_to_custom_classes(self, yolo_class):\n",
    "        \"\"\"\n",
    "        Map YOLO classes to our custom categories\n",
    "        \n",
    "        Args:\n",
    "            yolo_class (str): YOLO class name\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (custom_class, is_vehicle, is_person)\n",
    "        \"\"\"\n",
    "        # Vehicle mappings\n",
    "        vehicle_mapping = {\n",
    "            'car': 'car',\n",
    "            'truck': 'car',\n",
    "            'bus': 'car',\n",
    "            'bicycle': 'cycle',\n",
    "            'motorcycle': 'motorcycle'\n",
    "        }\n",
    "        \n",
    "        # Person mappings (YOLO doesn't distinguish gender/age, so we'll use additional logic)\n",
    "        person_mapping = {\n",
    "            'person': 'person'  # Will be further classified\n",
    "        }\n",
    "        \n",
    "        if yolo_class in vehicle_mapping:\n",
    "            return vehicle_mapping[yolo_class], True, False\n",
    "        elif yolo_class in person_mapping:\n",
    "            return self.classify_person_details(), False, True\n",
    "        \n",
    "        return None, False, False\n",
    "    \n",
    "    def classify_person_details(self):\n",
    "        \"\"\"\n",
    "        Classify person into male, female, or kid based on additional features\n",
    "        Note: This is a simplified approach. In production, you'd use a specialized model.\n",
    "        \"\"\"\n",
    "        # This is a placeholder implementation\n",
    "        # In a real system, you would use:\n",
    "        # 1. Gender classification model\n",
    "        # 2. Age estimation model\n",
    "        # 3. Height/feature analysis\n",
    "        \n",
    "        # For demo purposes, we'll use a random distribution\n",
    "        import random\n",
    "        choices = ['male', 'female', 'kid']\n",
    "        weights = [0.4, 0.4, 0.2]  # Weighted probabilities\n",
    "        return random.choices(choices, weights=weights)[0]\n",
    "    \n",
    "    def connect_to_camera(self):\n",
    "        \"\"\"Connect to RTSP stream\"\"\"\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(self.rtsp_url)\n",
    "            \n",
    "            # Set buffer size to minimize latency\n",
    "            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "            \n",
    "            if not self.cap.isOpened():\n",
    "                print(\"Error: Could not connect to camera\")\n",
    "                return False\n",
    "            \n",
    "            print(\"Successfully connected to CCTV camera\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to camera: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for detection\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame from camera\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (processed_frame, detection_data)\n",
    "        \"\"\"\n",
    "        if frame is None:\n",
    "            return frame, {}\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        results = self.model(rgb_frame)\n",
    "        \n",
    "        # Parse results\n",
    "        detection_data = self.parse_detections(results, frame.shape)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        processed_frame = self.draw_detections(frame, detection_data)\n",
    "        \n",
    "        return processed_frame, detection_data\n",
    "    \n",
    "    def parse_detections(self, results, frame_shape):\n",
    "        \"\"\"\n",
    "        Parse YOLO detection results\n",
    "        \n",
    "        Args:\n",
    "            results: YOLO detection results\n",
    "            frame_shape: Shape of the original frame\n",
    "            \n",
    "        Returns:\n",
    "            dict: Detection data organized by class\n",
    "        \"\"\"\n",
    "        detection_data = {cls: 0 for cls in self.vehicle_classes + self.person_classes}\n",
    "        detections = []\n",
    "        \n",
    "        if hasattr(results, 'xyxy') and len(results.xyxy) > 0:\n",
    "            for *xyxy, conf, cls in results.xyxy[0]:\n",
    "                class_name = results.names[int(cls)]\n",
    "                custom_class, is_vehicle, is_person = self.map_yolo_to_custom_classes(class_name)\n",
    "                \n",
    "                if custom_class:\n",
    "                    detection_data[custom_class] += 1\n",
    "                    detections.append({\n",
    "                        'class': custom_class,\n",
    "                        'bbox': [int(coord) for coord in xyxy],\n",
    "                        'confidence': float(conf),\n",
    "                        'is_vehicle': is_vehicle,\n",
    "                        'is_person': is_person\n",
    "                    })\n",
    "        \n",
    "        detection_data['detections'] = detections\n",
    "        return detection_data\n",
    "    \n",
    "    def draw_detections(self, frame, detection_data):\n",
    "        \"\"\"\n",
    "        Draw bounding boxes and labels on frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Original frame\n",
    "            detection_data: Detection information\n",
    "            \n",
    "        Returns:\n",
    "            frame: Frame with drawn detections\n",
    "        \"\"\"\n",
    "        for detection in detection_data.get('detections', []):\n",
    "            bbox = detection['bbox']\n",
    "            class_name = detection['class']\n",
    "            confidence = detection['confidence']\n",
    "            \n",
    "            color = self.colors.get(class_name, (255, 255, 255))\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n",
    "            \n",
    "            # Draw label background\n",
    "            label = f\"{class_name} {confidence:.2f}\"\n",
    "            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "            cv2.rectangle(frame, (bbox[0], bbox[1] - label_size[1] - 10),\n",
    "                         (bbox[0] + label_size[0], bbox[1]), color, -1)\n",
    "            \n",
    "            # Draw label text\n",
    "            cv2.putText(frame, label, (bbox[0], bbox[1] - 5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def display_statistics(self, frame, detection_data):\n",
    "        \"\"\"\n",
    "        Display detection statistics on frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame\n",
    "            detection_data: Detection information\n",
    "            \n",
    "        Returns:\n",
    "            frame: Frame with statistics overlay\n",
    "        \"\"\"\n",
    "        # Create statistics text\n",
    "        stats_text = []\n",
    "        \n",
    "        # Vehicle statistics\n",
    "        vehicle_count = sum(detection_data.get(cls, 0) for cls in self.vehicle_classes)\n",
    "        stats_text.append(f\"Vehicles: {vehicle_count}\")\n",
    "        for vehicle in self.vehicle_classes:\n",
    "            count = detection_data.get(vehicle, 0)\n",
    "            if count > 0:\n",
    "                stats_text.append(f\"  {vehicle}: {count}\")\n",
    "        \n",
    "        # Person statistics\n",
    "        person_count = sum(detection_data.get(cls, 0) for cls in self.person_classes)\n",
    "        stats_text.append(f\"Persons: {person_count}\")\n",
    "        for person in self.person_classes:\n",
    "            count = detection_data.get(person, 0)\n",
    "            if count > 0:\n",
    "                stats_text.append(f\"  {person}: {count}\")\n",
    "        \n",
    "        # Draw statistics panel\n",
    "        y_offset = 30\n",
    "        for i, text in enumerate(stats_text):\n",
    "            color = (0, 0, 0) if i % 2 == 0 else (50, 50, 50)\n",
    "            cv2.rectangle(frame, (10, y_offset - 20), (250, y_offset), (255, 255, 255), -1)\n",
    "            cv2.putText(frame, text, (15, y_offset - 5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "            y_offset += 25\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def run_detection(self):\n",
    "        \"\"\"Main function to run real-time detection\"\"\"\n",
    "        if not self.connect_to_camera():\n",
    "            return\n",
    "        \n",
    "        print(\"Starting real-time detection...\")\n",
    "        print(\"Press 'q' to quit, 'p' to pause\")\n",
    "        \n",
    "        paused = False\n",
    "        \n",
    "        while True:\n",
    "            if not paused:\n",
    "                ret, frame = self.cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"Error reading frame. Reconnecting...\")\n",
    "                    self.connect_to_camera()\n",
    "                    continue\n",
    "                \n",
    "                # Process frame\n",
    "                processed_frame, detection_data = self.process_frame(frame)\n",
    "                \n",
    "                # Display statistics\n",
    "                processed_frame = self.display_statistics(processed_frame, detection_data)\n",
    "                \n",
    "                # Display frame\n",
    "                cv2.imshow('CCTV Vehicle and Person Detection', processed_frame)\n",
    "            \n",
    "            # Handle key presses\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('p'):\n",
    "                paused = not paused\n",
    "                print(\"Paused\" if paused else \"Resumed\")\n",
    "        \n",
    "        # Cleanup\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Detection stopped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bdf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Priya/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-11-4 Python-3.11.2 torch-2.9.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Successfully connected to CCTV camera\n",
      "Starting real-time detection...\n",
      "Press 'q' to quit, 'p' to pause\n",
      "Error reading frame. Reconnecting...\n",
      "Successfully connected to CCTV camera\n",
      "Error reading frame. Reconnecting...\n",
      "Successfully connected to CCTV camera\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # RTSP URL format examples:\n",
    "    # rtsp://username:password@ip_address:port/stream\n",
    "    # rtsp://ip_address:554/stream\n",
    "    \n",
    "    # Replace with your actual RTSP URL\n",
    "    rtsp_url = \"rtsp://kumar:Kumar%23123@116.73.21.116:554/Streaming/channels/101\"\n",
    "    \n",
    "    # For testing with webcam (comment the above and uncomment below)\n",
    "    # rtsp_url = 0  # Uses default webcam\n",
    "    \n",
    "    # Initialize detection system\n",
    "    detector = CCTVDetection(rtsp_url)\n",
    "    \n",
    "    try:\n",
    "        # Start detection\n",
    "        detector.run_detection()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nDetection interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
